project_name: "coq-theorem-embedding"
experiment_name: "infonce-final-1"
log_level: "INFO"

dataset:
  dataset_path: "./data/"
  rankin_ds_path_statements: "./data/imm/basic/Events_statements.json"
  rankin_ds_path_references: "./sanityCheckSet/reference_premises.json"
  samples_from_single_anchor: 150
  train_split: 0.7
  val_split: 0.2
  test_split: 0.1

base_model_name: "microsoft/codebert-base"
max_seq_length: 128
embedding_dim: 768

# proof distance <= threshold -> positive
threshold_pos: 0.3
# proof distance >= threshold -> negative
threshold_neg: 0.65
# With some probability (30%), we sample a hard negative
# and treat it as a negative. It is done to smoothen the
# decision boundary and make the model more robust
threshold_hard_neg: 0.45
# In InfoNCE loss, one query is 1 positive and k negatives
# We maximize the similarity of the query with the
# positive and minimize it with the negatives
k_negatives: 4

steps: 22000
batch_size: 32
warmup_ratio: 0.1
learning_rate: 4e-6
random_seed: 52

wandb:
  enabled: false
  project: "coq-embeddings"
  entity: "********"
  tags: ["dyn-lr", "InfoNCE", "hard-negatives", "codebert"]

evaluation:
  output_dir: "./checkpoints/"
  save_freq: 1000
  eval_steps: 200
  evaluate_freq: 200
  # For precision@k and recall@k
  k_values: [5, 10]
  f_score_beta: 1
  # In evaluation stage we take an anchor sample,
  # and then {query_size_in_eval} other samples,
  # and rank them by their similarity to the anchor.
  # Afterward, estimate precision metrics
  query_size_in_eval: 20